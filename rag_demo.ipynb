{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import BM25Retriever\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_files(path: str) -> list[str]:\n",
    "    \n",
    "    with open(path, \"r\") as f:\n",
    "        contents = f.readlines() \n",
    "    return contents\n",
    "api_key = read_text_files(\"api.txt\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token = api_key)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", token = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_string = tokenizer.encode(\"I love apples.<eos>\")\n",
    "print(encoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_string = tokenizer.encode(\"I love apples.\", return_tensors= \"pt\") #encode\n",
    "output = model.generate(encoded_string, max_length = 128) \n",
    "print(f\"モデルの出力: {output}\")\n",
    "decoded_string = tokenizer.decode(output[0].tolist()) #decode\n",
    "print(f\"出力の文字列: {decoded_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id=\"google/gemma-2b-it\", model_kwargs={\"temperature\":0.5, \"max_length\":8192, \"topk\": 40}, huggingfacehub_api_token = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.predict(\"大谷翔平って誰\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SuperHotDogCat/Gemma-RAG-Handson/blob/main/gemma.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
